{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "295ae978-e76f-411d-9c72-04e80576fc26",
   "metadata": {},
   "source": [
    "# Classifying Images Using Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0755100-8294-4748-b491-66cc3edfe990",
   "metadata": {},
   "source": [
    "# Environment Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c067c-d424-4820-8b11-2a60b4122c2a",
   "metadata": {},
   "source": [
    "Please read the project's README for instructions on how to set up the project's environment on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04a484-50f8-40ee-81b3-159c61f58768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\t\n",
    "sys.executable  # Display the path to the Python executable ensuring the correct env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33bc23-0966-462c-88e7-f261ac124e43",
   "metadata": {},
   "source": [
    "# Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50924c43-eed2-491f-8b86-023886343363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # For numerical operations and arrays\t\t\t\t\n",
    "import matplotlib.pyplot as plt  # For basic plotting\t\t\t\n",
    "import tensorflow as tf  # For building and training ML models\t\t\t\n",
    "import tensorflow_datasets as tfds  # For accessing a wide range of prebuilt datasets\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "\n",
    "from python_scripts import batch_and_prefetch_datasets, create_image_model, train_eval_present_results, gathered_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7c04b-c92e-4cd4-b567-054702f84c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset using TensorFlow Datasets:\n",
    "# as_supervised=True returns a tuple (image, label) instead of a dictionary with 'image' and 'label' as keys\n",
    "# with_info=True returns additional information about the dataset\n",
    "mnist, mnist_info = tfds.load(name='mnist', as_supervised=True, with_info = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba49494d-7f92-4fb1-8deb-703c496df2f8",
   "metadata": {},
   "source": [
    "# Exploration of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9730e9b-762e-42bd-beed-8fa88b14a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of mnist:\", type(mnist))\n",
    "print(\"Type of mnist_info:\", type(mnist_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd57c7c-8c16-4b67-bcba-8a26bd4bd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604faf3a-e39d-4951-afa4-07147fac4b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist['train'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95775cee-2493-4b5f-b997-fdf45a4e7073",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "The ‘mnist’ dictionary contains 2 keys, ‘train’ and ‘test’, which means that the dataset is already divided in train and test sets. Further exploration to check for any subkeys inside the 'train' key showed that there are no additional subkeys, which implies that 'train' is not a nested dictionary but a dataset object.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8368a2a-39c2-49d9-963e-3ac46a4c8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df548259-2210-497e-b152-d14766d482f1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Observing the above result and already knowing that we load the dataset as tuples of two elements we can conclude that we see two TensorSpec instances in a tuple form and each one has its shape, its dtype and an empty name. The first TensorSpec is a third-rank tensor with shape (28, 28, 1) and the second one is a zero-rank tensor of shape (). The first element is the image and the second element is the corresponding image label which is a scalar.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039bdc65-829d-42d0-a415-9b0d77c8d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = next(iter(mnist['train']))  # Retrieve a single value from the 'train' key\n",
    "image, label = value  # Unpack the two element (image, label) value tuple\n",
    "plt.imshow(image.numpy().squeeze())  # Show the image converting it to a numpy array and removing the third dimension\n",
    "print(\"Label:\", label.numpy())  # Convert label to numpy array and show it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba1c07-8e42-4a7f-9a5f-fcc208e22bc1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "From the output above, we gained a better understandig of what each tuple contains. We retrieved a single element (a dictionary value) from the 'train' key. We then unpacked this element since it is a tuple consisting of two values: an image and its label and finally printed both the image and the label on screen after converting these values to NumPy arrays.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff32d34-f0a1-4cd7-ac74-81601de0b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the lengths of each prepared set:\n",
    "print('Train Set Length:', len(mnist['train']))  \n",
    "print('Test Set Length:', len(mnist['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d8d21-39af-473a-8072-14521368a15a",
   "metadata": {},
   "source": [
    "# Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4698b5-cace-41a8-82de-7edb25e5e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each dictionary key to variables creating the train and test sets.\n",
    "train_set, test_set = mnist['train'], mnist['test']\n",
    "\n",
    "# Assign the train set size to a variable:\n",
    "train_set_size = mnist_info.splits['train'].num_examples\n",
    "\n",
    "# Assign the validation set size to a variable (10% of the train set size):\n",
    "validation_set_size = int(train_set_size * 0.1)\n",
    "\n",
    "# Shuffle the entire train set:\n",
    "train_set = train_set.shuffle(buffer_size=train_set_size)\n",
    "\n",
    "# Create the validation set from the shuffled train set:\n",
    "validation_set = train_set.take(validation_set_size)\n",
    "\n",
    "# Subtract the validation set from the train set:\n",
    "train_set = train_set.skip(validation_set_size)\n",
    "\n",
    "# Create a manual image scaler. Be aware to add label although it won't be used:\n",
    "def scale_images(image, label):  # We add the parameter label just for format consistency\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label  # Having added the label, we can return a tuple to meet exactly the original dataset's format\n",
    "\n",
    "# Apply the image scaling function to each dataset:\n",
    "train_set, validation_set, test_set = (\n",
    "    train_set.map(scale_images),  # The function is applied to the train set.\n",
    "    validation_set.map(scale_images),  # The function is applied to the validation set.\n",
    "    test_set.map(scale_images))  # The function is applied to the test set.\n",
    "\n",
    "# Call a function which batches and prefetches the train, validation and test sets:\n",
    "train_set_1, validation_set_1, test_set_1, validation_set_inputs_1, validation_set_targets_1 = batch_and_prefetch_datasets(\n",
    "    train_set= train_set,\n",
    "    validation_set= validation_set,\n",
    "    test_set= test_set,\n",
    "    batch_size=100  # Hyperparameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b6dd9-8236-4bdd-b6af-d755a4d2bb3a",
   "metadata": {},
   "source": [
    "# Deep Learning Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c724761-a6f9-46c7-8415-ca3616e157dd",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "For the model creation, we'll call a function that accepts several parameters. We won't call it right now, but it can be seen by opening the python_scripts.py file with the name 'create_image_model'. The only reason we won't call it here is because we'll call it later inside another function to achieve unbiased results. That is, the model will be relaunched at the beginning of a for loop to initialize its weights every time a run cycle begins.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf74be2-3bcc-4c29-91b7-1aa86e630a65",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Concerning the model parameters, two of them are constants specific to the problem, while the rest are hyperparameters. A recommended approach is to use the Rectified Linear Unit (ReLU) activation function for introducing non-linearity in the hidden layers, and the softmax function for the final output layer, which produces probabilities for categorical classification. In this function, the only parameter that will vary is the list of hidden layer sizes, which is considered a hyperparameter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036df39f-d2ed-4718-b249-2b023d7bdecf",
   "metadata": {},
   "source": [
    "# Objective Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d40f029-e59c-4186-a069-e0541d432164",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Since this is a classification problem, we will use a cross-entropy loss function. Among the available cross-entropy loss functions, `sparse_categorical_crossentropy` is appropriate because the train set labels are not one-hot encoded. For the 'metrics' parameter, we will use 'accuracy' because MNIST is well-balanced across its 10 labels, and accuracy is a commonly used metric for classification problems.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3325636-4aac-4418-9e09-9ad3e7377684",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "We 'll use the Adaptive Moment Estimation (Adam) optimizer, as it is one of the most advanced optimization techniques available. By keeping all other hyperparameters constant, we will evaluate if using Adam results in better accuracy. Additionally, we will test the Stochastic Gradient Descent (SGD) optimizer with momentum as another potential option for this dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c2f44-c555-4300-9308-98d99a579173",
   "metadata": {},
   "source": [
    "# Baseline Model Training & Evaluation (This is Model Number 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c806d9-e1f3-4316-8603-ba3b6bb920b8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Creating and training a neural network can be complex, especially when it comes to optimizing the combination of hyperparameters. With numerous hyperparameters and their potential ranges, the number of possible combinations can be immense. This stage is often the most challenging part of the machine learning process.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ef2542-ea60-40c7-adb7-93d72dba6dcb",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "While there are techniques to automate and streamline this process, we will use a manual trial-and-error approach to evaluate how different hyperparameters impact the accuracy metric. This approach will help us understand the effect of various hyperparameters on model performance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd2e95-ac84-4d4d-9e2f-8e1a2c53b8db",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "The key function for performing unbiased test cycles, including training, validation, evaluation, and displaying the results on screen, is named 'gathered_information' and is located in the corresponding project Python file. We 'll use this function in the cell below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bed5ca-b74a-462c-9058-9475a23518ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the constants and hyperparameters for the model:\n",
    "INPUT_SHAPE = (28, 28, 1)  # Constant specific to this problem\n",
    "OUTPUT_SIZE = 10  # Constant specific to this problem\n",
    "HIDDEN_LAYER_SIZES = [50, 50]  # Hyperparameter: Number of neurons in each hidden layer\n",
    "ACTIVATION_FUNCTION = 'relu'  # Hyperparameter: We 'll keep it constant\n",
    "ACTIVATION_FUNCTION_OUTPUT = 'softmax'  # Hyperparameter: We 'll keep it constant\n",
    "N_RANGE=10  # Number of test cycles to ensure that the evaluation metrics are correct\n",
    "EPOCHS = 5  # Hyperparameter: Number of training epochs\n",
    "OPTIMIZER = 'adam'  # Hyperparameter: Optimization technique\n",
    "LEARNING_RATE = 0.001  # Hyperparameter: Learning rate for the above optimizer \n",
    "MOMENTUM = None  # Hyperparameter: Not used with 'adam' optimizer\n",
    "\n",
    "# Call the function to train and evaluate the model with the specified parameters\n",
    "summary_model_1 = gathered_information(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    hidden_layer_sizes=HIDDEN_LAYER_SIZES,\n",
    "    activation_fun=ACTIVATION_FUNCTION,\n",
    "    activation_fun_output=ACTIVATION_FUNCTION_OUTPUT,\n",
    "    n_range=N_RANGE,  \n",
    "    train_set=train_set_1,  # Training dataset\n",
    "    validation_inputs=validation_set_inputs_1,  # Validation dataset inputs\n",
    "    validation_targets=validation_set_targets_1,  # Validation dataset targets\n",
    "    test_set=test_set_1,  # Test dataset\n",
    "    epochs=EPOCHS,  \n",
    "    optimizer=OPTIMIZER,  \n",
    "    learn_rate=LEARNING_RATE,  \n",
    "    mom=MOMENTUM\n",
    ")\n",
    "\n",
    "# Display the resulting DataFrame with evaluation parameters:\n",
    "summary_model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb99203-5fe6-48cc-b857-5aa7a496a6a5",
   "metadata": {},
   "source": [
    "# Best Model with Adaptive Moment Estimation (Model 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0054b-9640-4af2-a299-59a9b4bcf3c4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "In this section, we focus on the second model, which utilizes Adaptive Moment Estimation (Adam) as the optimization technique. The process for training and evaluating this model is identical to the baseline model, with the key difference being the hyperparameter values. Specifically, we use different hyperparameter settings to optimize the model's performance. Comments have been omitted here for brevity, as the methodology remains consistent with that of the baseline model, with adjustments only to the hyperparameters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad2448-32a1-48f3-ba3c-88acfb6d3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_2, validation_set_2, test_set_2, validation_set_inputs_2, validation_set_targets_2 = batch_and_prefetch_datasets(\n",
    "    train_set= train_set,\n",
    "    validation_set= validation_set,\n",
    "    test_set= test_set,\n",
    "    batch_size=60\n",
    ")\n",
    "\n",
    "INPUT_SHAPE = (28, 28, 1) \n",
    "OUTPUT_SIZE = 10  \n",
    "HIDDEN_LAYER_SIZES = [500, 500, 500]  \n",
    "ACTIVATION_FUNCTION = 'relu'  \n",
    "ACTIVATION_FUNCTION_OUTPUT = 'softmax'\n",
    "N_RANGE=10 \n",
    "EPOCHS = 15 \n",
    "OPTIMIZER = 'adam' \n",
    "LEARNING_RATE = 0.001 \n",
    "MOMENTUM = None \n",
    "\n",
    "summary_model_2 = gathered_information(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    hidden_layer_sizes=HIDDEN_LAYER_SIZES,\n",
    "    activation_fun=ACTIVATION_FUNCTION,\n",
    "    activation_fun_output=ACTIVATION_FUNCTION_OUTPUT,\n",
    "    n_range=N_RANGE,  \n",
    "    train_set=train_set_2, \n",
    "    validation_inputs=validation_set_inputs_2, \n",
    "    validation_targets=validation_set_targets_2, \n",
    "    test_set=test_set_2, \n",
    "    epochs=EPOCHS, \n",
    "    optimizer=OPTIMIZER, \n",
    "    learn_rate=LEARNING_RATE, \n",
    "    mom=MOMENTUM  \n",
    ")\n",
    "\n",
    "summary_model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa3ce9-600e-49c7-8c1c-8cfb9ade60ff",
   "metadata": {},
   "source": [
    "# Best Model with Stochastic Gradient Descent and Momentum (Model 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a5915-e0c7-4edb-a7ae-8c7c6242029d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "In this section, we introduce the third model, which employs Stochastic Gradient Descent (SGD) with momentum as the optimization technique. This model follows the same training and evaluation process as the previous models, but with different hyperparameter settings specific to SGD. Momentum is included to potentially improve convergence speed and model performance. As with the previous models, detailed comments have been omitted for brevity, as the methodology remains consistent, with variations only in the hyperparameters and optimization approach.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e19984-a268-4cb1-8f9c-e11cc7f1531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (28, 28, 1) \n",
    "OUTPUT_SIZE = 10  \n",
    "HIDDEN_LAYER_SIZES = [600, 600, 600]  \n",
    "ACTIVATION_FUNCTION = 'relu'  \n",
    "ACTIVATION_FUNCTION_OUTPUT = 'softmax'\n",
    "N_RANGE=10 \n",
    "EPOCHS = 17 \n",
    "OPTIMIZER = 'sgd' \n",
    "LEARNING_RATE = 0.0013\n",
    "MOMENTUM = 0.99\n",
    "\n",
    "summary_model_3 = gathered_information(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    hidden_layer_sizes=HIDDEN_LAYER_SIZES,\n",
    "    activation_fun=ACTIVATION_FUNCTION,\n",
    "    activation_fun_output=ACTIVATION_FUNCTION_OUTPUT,\n",
    "    n_range=N_RANGE,  \n",
    "    train_set=train_set_2, \n",
    "    validation_inputs=validation_set_inputs_2, \n",
    "    validation_targets=validation_set_targets_2, \n",
    "    test_set=test_set_2, \n",
    "    epochs=EPOCHS, \n",
    "    optimizer=OPTIMIZER, \n",
    "    learn_rate=LEARNING_RATE, \n",
    "    mom=MOMENTUM  \n",
    ")\n",
    "\n",
    "summary_model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fc2f5-dc3e-4221-9d8d-cf64a102a0a2",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23054b7f-4195-475a-8be7-c93a7f5a9114",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "This project aimed to evaluate and compare different optimization techniques for training a neural network model for image classification. Specifically, it focused on assessing the performance of the baseline Adam optimizer, a tuned Adam optimizer, and the SGD optimizer. The key metrics used for comparison were average test accuracy, accuracy standard deviation, average test loss, and loss standard deviation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd08454-eba1-47ae-b226-200793161913",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "The model trained with the SGD optimizer outperformed both the baseline and tuned Adam models. It achieved the highest average test accuracy of 0.9841 and the lowest average test loss of 0.067417. Additionally, the accuracy and loss standard deviations were lower for SGD, indicating more stable and consistent performance across the runs.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
