# mnist_neural_networks
A comparative analysis of neural network optimizers for image classification, highlighting the performance improvements achieved through hyperparameter tuning.

I have conducted University research over the past years, particularly in mathematical modelling on published scientific data. Consequently, I am somewhat familiar with trial-and-error processes, which are important for hyperparameter tuning in the field of Data Science. 

I wanted to focus on deep learning and tensorflow and a well-understood and cleaned dataset already prepared in Tensorflow Datasets was an ideal opportunity to achieve my goal.

What I gained from this project:
1) Finding the correct parameter combinations is probably the most chalenging part in machine and deep learning. The same holds for academic research because the process has exactly the same roots.
2) I believe I managed to be unbiased by initializing the weight factors prior to each cycle run.
3) I built some useful functions to automate the process and gain precious time, demonstrating some modular programming skills.
4) I conducted a comparative analysis of two popular optimizers, Adam and SGD.

I finally achieved 98.41% test accuracy score with tuned SGD!

Thank you for your reading! 
